{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd96363b",
   "metadata": {},
   "source": [
    "### Vamos começar a analise exploratória de dados carregando o dataset para criar o dataframe pandas e verificar a quantidade de linhas e colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bf5319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação das bibliotecas utilizadas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from scipy.stats import mannwhitneyu, chi2_contingency\n",
    "\n",
    "# Carregando o arquivo csv e criando o DataFrame pandas\n",
    "df = pd.read_csv('_data/dataset_2021-5-26-10-14.csv', sep= '\\t', encoding='utf-8')\n",
    "\n",
    "# Verificando a quantidade de linhas e colunas do DataFrame\n",
    "print(f\"Quantidade de linhas e colunas: {df.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd6ecfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração utlizada para exibir todas as colunas ao executar o df.head()\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Ajusta a exibição dos valores para duas casas decimais\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "#Exibe as 5 primeiras linas do DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c05a077",
   "metadata": {},
   "source": [
    "### Sabemos que os valores faltantes estão marcados como missing, então vamos substituir tudo que for missing por nan para facilitar o tratamento a partir dos métodos do Pandas e Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4941bb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Substitui os valores missing por nan\n",
    "df.replace('missing', np.nan, inplace=True)\n",
    "\n",
    "# Exibe informações gerais do DataFrame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e01a5b",
   "metadata": {},
   "source": [
    "### É possível ver que temos valores faltantes em quatro colunas, então vamos verificar a quantidade ausente em cada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b26e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soma a quantidade de valores ausentes em cada coluna e coloca em ordem decrescente\n",
    "valores_ausentes = df.isna().sum().sort_values(ascending=False)\n",
    "\n",
    "# Exibe apenas as colunas com valores ausentes e suas respectivas quantidades\n",
    "print(f\"{valores_ausentes[valores_ausentes > 0]} \\n\")\n",
    "\n",
    "# Proporção de valores ausentes\n",
    "print((valores_ausentes[valores_ausentes > 0] / df.shape[0]).apply(lambda x: f\"{x:.3f}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353e191f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numerico = df.select_dtypes(include=np.number)\n",
    "\n",
    "# Contando valores menores que zero por coluna\n",
    "negativos_por_coluna = (df_numerico < 0).sum()\n",
    "\n",
    "print(\"Quantidade de valores menores que zero por coluna:\")\n",
    "print(negativos_por_coluna)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8103088f",
   "metadata": {},
   "source": [
    "### Aqui foram identificados 144 valores negativos em valor_total_pedido, o que não faz sentido, então esses valores deverão ser tratados posteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff9867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar valores unicos em cada variavel\n",
    "valores_unicos = []\n",
    "print(\"Quantidade de valores únicos em cada coluna:\")\n",
    "for col in df.columns.tolist():\n",
    "    print(f\"{col} : {len(df[col].value_counts())}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f0c022",
   "metadata": {},
   "source": [
    "### A variável \"participacao_falencia_valor\" só possui um valor para todo o dataset, dessa forma já vamos descartá-la de outras análises, pois tendo apenas um valor ela não ajuda a resolver o problema de classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a7d551",
   "metadata": {},
   "source": [
    "### Verificando a quantidade de valores unicos em cada variável é possivel notar que dentre as categóricas temos duas com grandes quantidades de categoria, o que pode ser problemático, pois elas resultariam num aumento da dimensionalidade e com isso prejudicar a performance do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812a3c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principais medidas estatísticas das variáveis numéricas\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2a5db6",
   "metadata": {},
   "source": [
    "### Verificando as variáveis continuas vemos que muitas delas possuem grande parte dos valores concentrados em zero, mas com valores máximos bem distantes, ou seja, há indicio da presença de muitos outliers que serão analisados e tratados para reduzir seu impacto no modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94c1a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estatísticas para variáveis categóricas\n",
    "df.describe(include=[\"object\"]).T.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bba0ab",
   "metadata": {},
   "source": [
    "### Nesta seção vamos análisar a proporção da variável alvo com relação a quantidade de linhas com valores nulos para saber se a diferença entre as linhas totais e a quantidade ao excluir as linhas com NaNs é estatisticamente significativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f555e8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "alvo = \"default\"\n",
    "\n",
    "print(f\"Distribuição total \\n {df[alvo].value_counts(), df[alvo].value_counts(normalize=True)} \\n\")\n",
    "\n",
    "# separa grupos\n",
    "df_nan = df[df.isna().any(axis=1)]      # linhas com pelo menos 1 NaN\n",
    "df_no_nan = df[~df.isna().any(axis=1)]  # linhas sem nenhum NaN\n",
    "\n",
    "# contagem absoluta\n",
    "print(f\"Com NaN:\\n {df_nan[alvo].value_counts(), df_nan[alvo].value_counts(normalize=True).apply(lambda x: f\"{x:.3f}\")} \\n\")\n",
    "\n",
    "# Proporção\n",
    "print(f\"Sem NaN:\\n {df_no_nan[alvo].value_counts(), df_no_nan[alvo].value_counts(normalize=True).apply(lambda x: f\"{x:.3f}\")} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a5441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar o número total de linhas duplicadas\n",
    "num_duplicadas = df.duplicated().sum()\n",
    "print(f\"Número de linhas duplicadas: {num_duplicadas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2439b751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contagem total da variavel alvo\n",
    "total_counts = df[alvo].value_counts()\n",
    "\n",
    "# com e sem NaN\n",
    "nan_counts = df[df.isna().any(axis=1)][alvo].value_counts()\n",
    "no_nan_counts = df[~df.isna().any(axis=1)][alvo].value_counts()\n",
    "\n",
    "# monta tabela de contingência\n",
    "matriz_de_contingencia = pd.DataFrame({\n",
    "    \"Com_NaN\": nan_counts,\n",
    "    \"Sem_NaN\": no_nan_counts,\n",
    "    \"Total\": total_counts\n",
    "}).fillna(0).astype(int)\n",
    "\n",
    "print(\"Matriz de contingência:\")\n",
    "print(matriz_de_contingencia, \"\\n\")\n",
    "\n",
    "# Teste qui-quadrado\n",
    "chi2, p, dof, expected = chi2_contingency(matriz_de_contingencia[[\"Com_NaN\",\"Sem_NaN\"]])\n",
    "\n",
    "print(f\"Qui-quadrado: {chi2:.4f}\")\n",
    "print(f\"p-valor: {p:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587205e3",
   "metadata": {},
   "source": [
    "### Como o p valor ficou abaixo de 0.05 há uma significancia estatistica de associação entre as linhas com valores nulos e a variavel alvo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeb096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleciona as variáveis numéricas e exclui \"participacao_falencia_valor\"\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.drop(alvo).drop(\"participacao_falencia_valor\")\n",
    "\n",
    "# Correlação entre as variáveis\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(df[num_cols].corr(), annot= True, fmt='.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1a9f1b",
   "metadata": {},
   "source": [
    "### Com auxilio dessa matriz podemos ver que nenhuma variável é reduntante (correlação acima de 0.8), principalmente para modelos baseados em árvores, porém vamos realizar uma análise unidimensional para decidir se vamos manter todas as variáveis ou não, pois ao reduzir a quantidade de variáveis diminuimos o custo computacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0623080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [20.00, 15.00]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "f, eixos = plt.subplots(4,4)\n",
    "\n",
    "linha, coluna = 0, 0\n",
    "\n",
    "for i in num_cols:\n",
    "    sns.boxplot(data=df,y=i, ax=eixos[linha][coluna])\n",
    "    coluna += 1\n",
    "    if coluna == 4:\n",
    "        linha += 1\n",
    "        coluna = 0\n",
    "        if linha == 4:   \n",
    "            break\n",
    "                \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fada6cc",
   "metadata": {},
   "source": [
    "### Acima podemos ver que há uma grande quantidade de outliers em diversas variáveis então vamos avaliá-las para entender esses valores e decidir como tratar os mesmos. Também vamos aplicar o teste de Mann-Whitney para tentar descobrir quais variáveis tem mais poder discriminatório entre quem dá default e quem não dá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64a620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função que recebe uma coluna do df e retorna informações como limite superior e inferior calculado a partir da distancia interquartil,\n",
    "# assim como também apresenta a quantidade de valores ausentes, resultado de testes estatísticos e graficos referentes a cada variavel.\n",
    "def analise_continuas(coluna):\n",
    "    print (f\"Coluna: {coluna}\")\n",
    "    print(f\"Quantidade de valores unicos: {len(df[coluna].unique())}\")\n",
    "    df_desc = df[coluna].describe().T\n",
    "    DQ = df_desc[\"75%\"] -  df_desc[\"25%\"]\n",
    "    limite_inf = df_desc[\"25%\"] - 1.5*DQ\n",
    "    limite_sup = df_desc[\"75%\"] + 1.5*DQ\n",
    "    outliers = df[(df[coluna] > limite_sup) | (df[coluna] < limite_inf)] [[coluna, alvo]]\n",
    "    num_valores_ausentes = df[coluna].isna().sum()\n",
    "    \n",
    "    \n",
    "    print(f\"Limite Inferior: {limite_inf}\")\n",
    "    print(f\"Limite Superior: {limite_sup}\")\n",
    "    print(f\"Quantidade de Outliers: {outliers.shape[0]} \")\n",
    "    print(f\"Número de valores ausentes na variável {coluna}: {num_valores_ausentes} \\n\")\n",
    "    \n",
    "    # Testes estatísticos\n",
    "    grupo_1 = df [(df[alvo] == 0) & (~df[coluna].isna())][coluna]\n",
    "    grupo_2 = df [(df[alvo] == 1) & (~df[coluna].isna())][coluna]\n",
    "\n",
    "    statistic, p_value = mannwhitneyu(grupo_1, grupo_2)\n",
    "    print(f\"p valor do teste de Mann-Whitney : {p_value:.8f} \\n\\n\")\n",
    "    \n",
    "    # Graficos\n",
    "    \n",
    "    fig = px.histogram(df, x=coluna, color=alvo, nbins=20, barmode=\"relative\", title=f\"Distribuição da variável {coluna} por {alvo}\")\n",
    "    fig.update_layout(width=600, height=500)\n",
    "    fig.show()\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.scatterplot(data=df, x=coluna, y=alvo)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d90dbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chama a função para todas as colunas numericas\n",
    "for colunas in num_cols:\n",
    "    analise_continuas(colunas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe7061c",
   "metadata": {},
   "source": [
    "### Antes de decidir o que fazer com outliers é uma boa prática consultar a área de negócios para entender se aqueles valores de fato fazem sentido ou se realmente se tratam de erros, porém para resolver o exercícios vamos utilizar a técnica de substituir os outliers pelo limite superior (quando esse limite for diferente de 0) e para os valores negativos vamos substituir por 0. Outra ação que será tomada é a transformação logaritmica das variáveis com valores máximos muito altos para reduzir o peso deles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b949b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função que analisa as variáveis categóricas\n",
    "def analise_categoricas(coluna):\n",
    "    print(coluna)\n",
    "    print(f\"{str(df[coluna].isna().sum())} ausentes\")\n",
    "    \n",
    "    matriz_contingencia = pd.crosstab(df[coluna], df[alvo])\n",
    "    \n",
    "    chi2, p, dof, expected = chi2_contingency(matriz_contingencia)\n",
    "    print(f\"Teste Qui-Quadrado {chi2}\")\n",
    "    print(f\"p valor: {p} \\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dab1a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "for coluna in df.select_dtypes(include=object):\n",
    "    analise_categoricas(coluna)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c183a6",
   "metadata": {},
   "source": [
    "### Para as variáveis categóricas foi aplicado o Teste Qui-Quadrado a fim de descobrir se a associação entre elas e o alvo são estatisticamente significantes, todas tiveram p valor menores que 0.05, mostrando associação com o target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9c4fe5",
   "metadata": {},
   "source": [
    "### Como o Teste Mann-Whitney e o Qui-Quadrado não nos ajudaram a escolher quais as variáveis mais importantes, uma vez que o p valor deu menor que 0.05 para todas as variáveis (ocorrencia comum em datasets grandes), vamos analisar a Área sob a curva ROC para ver quais variáveis performam melhor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4703cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "resultados = []\n",
    "\n",
    "for col in num_cols:\n",
    "    try:\n",
    "        # Calcula a area sob a curva\n",
    "        auc = roc_auc_score(df[alvo], df[col])\n",
    "            \n",
    "        resultados.append((col, auc))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro em {col}: {e}\")\n",
    "\n",
    "# Monta DataFrame de resultados\n",
    "df_resultados = pd.DataFrame(resultados, columns=[\"variavel\", \"auc\"]) \\\n",
    "                 .sort_values(by=\"auc\", ascending=False)\n",
    "\n",
    "print(df_resultados.head(16))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d54efb",
   "metadata": {},
   "source": [
    "### A análise da área sob a curva ROC nos trouxe algumas variáveis que sozinhas tem um poder discriminatório maior, ainda que não muito grande, mas já podemos identificar algumas que devem ter no modelo final, sendo elas as com auc > 0.55. As variáveis com valor = 0.5 não possuem nenhum poder discriminativo univariado, na pratica seria equivalente ao classificar ao acaso, mas combinado com outras variáveis podem ser uteis para o modelo então não serão totalmente descartadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311be6d3",
   "metadata": {},
   "source": [
    "## Resumo das análises \n",
    "\n",
    "- Dataset grande com 117273 linhas e 22 colunas ao todo, sem linhas duplicadas, 4 variáveis categóricas e 18 contínuas;\n",
    "- Grande quantidade de Outliers que, quando aplicavel, serão tratados com imputação;\n",
    "- Variável \"participacao_falencia_valor\" descartada por possuir apenas um valor em todo o dataset;\n",
    "- Variáveis contínuas com grande intervalo de valores, necessário aplicar logaritmo para amenizar o impacto de valores extremos;\n",
    "- Valores negativos na coluna \"valor_total_pedido\" serão substituidos por 0;\n",
    "- Os valores ausentes possuem significancia estatistica de associação com a variável \"default\" então não serão excluídos. É esperado que o algoritmo aprenda se existir alguma relação entre o valor ausente e a variável desfecho;\n",
    "- Variáveis com maior poder discriminativo: default_3months, ioi_36months, valor_vencido e valor_total_pedido.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
